
# Task-01 Text Generation

## Objective
To fine-tune a pre-trained GPT-2 transformer model for generating coherent and context-aware text.

## Description
This project demonstrates text generation using a pre-trained GPT-2 model fine-tuned on a custom Hinglish dataset. The model generates human-like responses based on given prompts.

## Technologies Used
- Python
- PyTorch
- Hugging Face Transformers
- GPT-2

## Files Included
- `main.py` – Model training script
- `test.py` – Text generation script
- `data.txt` – Custom Hinglish dataset

## Result
The fine-tuned model successfully generates meaningful text responses aligned with the dataset style.
